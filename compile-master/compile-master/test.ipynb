{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_toy_data(num_symbols=5, num_segments=3, max_segment_len=5):\n",
    "    \"\"\"Generate toy data sample with repetition of symbols (EOS symbol: 0).\"\"\"\n",
    "    seq = []\n",
    "    symbols = np.random.choice(\n",
    "        np.arange(1, num_symbols + 1), num_segments, replace=False)\n",
    "    for seg_id in range(num_segments):\n",
    "        segment_len = np.random.choice(np.arange(1, max_segment_len))\n",
    "        seq += [symbols[seg_id]] * segment_len\n",
    "    seq += [0]\n",
    "    return torch.tensor(seq, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "EPS = 1e-17\n",
    "NEG_INF = -1e30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 4, 4,  ..., 0, 0, 0],\n",
      "        [3, 3, 3,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [2, 2, 5,  ..., 0, 0, 0],\n",
      "        [4, 4, 5,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for _ in range(512):\n",
    "    data.append(generate_toy_data(\n",
    "        num_symbols=5,\n",
    "        num_segments=3))\n",
    "lengths = torch.tensor(list(map(len, data)))\n",
    "inputs = torch.nn.utils.rnn.pad_sequence(data, batch_first=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Utility functions.\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "EPS = 1e-17\n",
    "NEG_INF = -1e30\n",
    "\n",
    "\n",
    "def to_one_hot(indices, max_index):\n",
    "    \"\"\"Get one-hot encoding of index tensors.\"\"\"\n",
    "    zeros = torch.zeros(\n",
    "        indices.size()[0], max_index, dtype=torch.float32,\n",
    "        device=indices.device)\n",
    "    return zeros.scatter_(1, indices.unsqueeze(1), 1)\n",
    "\n",
    "\n",
    "def gumbel_sample(shape):\n",
    "    \"\"\"Sample Gumbel noise.\"\"\"\n",
    "    uniform = torch.rand(shape).float()\n",
    "    return - torch.log(EPS - torch.log(uniform + EPS))\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temp=1.):\n",
    "    \"\"\"Sample from the Gumbel softmax / concrete distribution.\"\"\"\n",
    "    gumbel_noise = gumbel_sample(logits.size())\n",
    "    if logits.is_cuda:\n",
    "        gumbel_noise = gumbel_noise.cuda()\n",
    "    return F.softmax((logits + gumbel_noise) / temp, dim=-1)\n",
    "\n",
    "\n",
    "def gaussian_sample(mu, log_var):\n",
    "    \"\"\"Sample from Gaussian distribution.\"\"\"\n",
    "    gaussian_noise = torch.randn(mu.size())\n",
    "    if mu.is_cuda:\n",
    "        gaussian_noise = gaussian_noise.cuda()\n",
    "    return mu + torch.exp(log_var * 0.5) * gaussian_noise\n",
    "\n",
    "\n",
    "def kl_gaussian(mu, log_var):\n",
    "    \"\"\"KL divergence between Gaussian posterior and standard normal prior.\"\"\"\n",
    "    return -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), dim=1)\n",
    "\n",
    "\n",
    "def kl_categorical_uniform(preds):\n",
    "    \"\"\"KL divergence between categorical distribution and uniform prior.\"\"\"\n",
    "    kl_div = preds * torch.log(preds + EPS)  # Constant term omitted.\n",
    "    return kl_div.sum(1)\n",
    "\n",
    "\n",
    "def kl_categorical(preds, log_prior):\n",
    "    \"\"\"KL divergence between two categorical distributions.\"\"\"\n",
    "    kl_div = preds * (torch.log(preds + EPS) - log_prior)\n",
    "    return kl_div.sum(1)\n",
    "\n",
    "\n",
    "def poisson_categorical_log_prior(length, rate, device):\n",
    "    \"\"\"Categorical prior populated with log probabilities of Poisson dist.\"\"\"\n",
    "    rate = torch.tensor(rate, dtype=torch.float32, device=device)\n",
    "    values = torch.arange(\n",
    "        1, length + 1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    log_prob_unnormalized = torch.log(\n",
    "        rate) * values - rate - (values + 1).lgamma()\n",
    "    # TODO(tkipf): Length-sensitive normalization.\n",
    "    return F.log_softmax(log_prob_unnormalized, dim=1)  # Normalize.\n",
    "\n",
    "\n",
    "def log_cumsum(probs, dim=1):\n",
    "    \"\"\"Calculate log of inclusive cumsum.\"\"\"\n",
    "    return torch.log(torch.cumsum(probs, dim=dim) + EPS)\n",
    "\n",
    "\n",
    "def generate_toy_data(num_symbols=5, num_segments=3, max_segment_len=5):\n",
    "    \"\"\"Generate toy data sample with repetition of symbols (EOS symbol: 0).\"\"\"\n",
    "    seq = []\n",
    "    symbols = np.random.choice(\n",
    "        np.arange(1, num_symbols + 1), num_segments, replace=False)\n",
    "    for seg_id in range(num_segments):\n",
    "        segment_len = np.random.choice(np.arange(1, max_segment_len))\n",
    "        seq += [symbols[seg_id]] * segment_len\n",
    "    seq += [0]\n",
    "    return torch.tensor(seq, dtype=torch.int64)\n",
    "\n",
    "\n",
    "def get_lstm_initial_state(batch_size, hidden_dim, device):\n",
    "    \"\"\"Get empty (zero) initial states for LSTM.\"\"\"\n",
    "    hidden_state = torch.zeros(batch_size, hidden_dim, device=device)\n",
    "    cell_state = torch.zeros(batch_size, hidden_dim, device=device)\n",
    "    return hidden_state, cell_state\n",
    "\n",
    "\n",
    "def get_segment_probs(all_b_samples, all_masks, segment_id):\n",
    "    \"\"\"Get segment probabilities for a particular segment ID.\"\"\"\n",
    "    neg_cumsum = 1 - torch.cumsum(all_b_samples[segment_id], dim=1)\n",
    "    if segment_id > 0:\n",
    "        return neg_cumsum * all_masks[segment_id - 1]\n",
    "    else:\n",
    "        return neg_cumsum\n",
    "\n",
    "\n",
    "def get_losses(inputs, outputs, args, beta_b=.1, beta_z=.1, prior_rate=3.,):\n",
    "    \"\"\"Get losses (NLL, KL divergences and neg. ELBO).\n",
    "\n",
    "    Args:\n",
    "        inputs: Padded input sequences.\n",
    "        outputs: CompILE model output tuple.\n",
    "        args: Argument dict from `ArgumentParser`.\n",
    "        beta_b: Scaling factor for KL term of boundary variables (b).\n",
    "        beta_z: Scaling factor for KL term of latents (z).\n",
    "        prior_rate: Rate (lambda) for Poisson prior.\n",
    "    \"\"\"\n",
    "\n",
    "    targets = inputs.view(-1)\n",
    "    all_encs, all_recs, all_masks, all_b, all_z = outputs\n",
    "    input_dim = args.num_symbols + 1\n",
    "\n",
    "    nll = 0.\n",
    "    kl_z = 0.\n",
    "    for seg_id in range(args.num_segments):\n",
    "        seg_prob = get_segment_probs(\n",
    "            all_b['samples'], all_masks, seg_id)\n",
    "        preds = all_recs[seg_id].view(-1, input_dim)\n",
    "        seg_loss = F.cross_entropy(\n",
    "            preds, targets, reduction='none').view(-1, inputs.size(1))\n",
    "\n",
    "        # Ignore EOS token (last sequence element) in loss.\n",
    "        nll += (seg_loss[:, :-1] * seg_prob[:, :-1]).sum(1).mean(0)\n",
    "\n",
    "        # KL divergence on z.\n",
    "        if args.latent_dist == 'gaussian':\n",
    "            mu, log_var = torch.split(\n",
    "                all_z['logits'][seg_id], args.latent_dim, dim=1)\n",
    "            kl_z += kl_gaussian(mu, log_var).mean(0)\n",
    "        elif args.latent_dist == 'concrete':\n",
    "            kl_z += kl_categorical_uniform(\n",
    "                F.softmax(all_z['logits'][seg_id], dim=-1)).mean(0)\n",
    "        else:\n",
    "            raise ValueError('Invalid argument for `latent_dist`.')\n",
    "\n",
    "    # KL divergence on b (first segment only, ignore first time step).\n",
    "    # TODO(tkipf): Implement alternative prior on soft segment length.\n",
    "    probs_b = F.softmax(all_b['logits'][0], dim=-1)\n",
    "    log_prior_b = poisson_categorical_log_prior(\n",
    "        probs_b.size(1), prior_rate, device=inputs.device)\n",
    "    kl_b = args.num_segments * kl_categorical(\n",
    "        probs_b[:, 1:], log_prior_b[:, 1:]).mean(0)\n",
    "\n",
    "    loss = nll + beta_z * kl_z + beta_b * kl_b\n",
    "    return loss, nll, kl_z, kl_b\n",
    "\n",
    "\n",
    "def get_reconstruction_accuracy(inputs, outputs, args):\n",
    "    \"\"\"Calculate reconstruction accuracy (averaged over sequence length).\"\"\"\n",
    "\n",
    "    all_encs, all_recs, all_masks, all_b, all_z = outputs\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "\n",
    "    rec_seq = []\n",
    "    rec_acc = 0.\n",
    "    for sample_idx in range(batch_size):\n",
    "        prev_boundary_pos = 0\n",
    "        rec_seq_parts = []\n",
    "        for seg_id in range(args.num_segments):\n",
    "            boundary_pos = torch.argmax(\n",
    "                all_b['samples'][seg_id], dim=-1)[sample_idx]\n",
    "            if prev_boundary_pos > boundary_pos:\n",
    "                boundary_pos = prev_boundary_pos\n",
    "            seg_rec_seq = torch.argmax(all_recs[seg_id], dim=-1)\n",
    "            rec_seq_parts.append(\n",
    "                seg_rec_seq[sample_idx, prev_boundary_pos:boundary_pos])\n",
    "            prev_boundary_pos = boundary_pos\n",
    "        rec_seq.append(torch.cat(rec_seq_parts))\n",
    "        cur_length = rec_seq[sample_idx].size(0)\n",
    "        matches = rec_seq[sample_idx] == inputs[sample_idx, :cur_length]\n",
    "        rec_acc += matches.float().mean()\n",
    "    rec_acc /= batch_size\n",
    "    return rec_acc, rec_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import utils\n",
    "\n",
    "\n",
    "class CompILE(nn.Module):\n",
    "    \"\"\"CompILE example implementation.\n",
    "\n",
    "    Args:\n",
    "        input_dim: Dictionary size of embeddings.\n",
    "        hidden_dim: Number of hidden units.\n",
    "        latent_dim: Dimensionality of latent variables (z).\n",
    "        max_num_segments: Maximum number of segments to predict.\n",
    "        temp_b: Gumbel softmax temperature for boundary variables (b).\n",
    "        temp_z: Temperature for latents (z), only if latent_dist='concrete'.\n",
    "        latent_dist: Whether to use Gaussian latents ('gaussian') or concrete /\n",
    "            Gumbel softmax latents ('concrete').\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, max_num_segments,\n",
    "                 temp_b=1., temp_z=1., latent_dist='gaussian'):\n",
    "        super(CompILE, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_num_segments = max_num_segments\n",
    "        self.temp_b = temp_b\n",
    "        self.temp_z = temp_z\n",
    "        self.latent_dist = latent_dist\n",
    "        print('input_dim:', input_dim)\n",
    "        print('hidden_dim:', hidden_dim)\n",
    "        print('latent_dim', latent_dim)\n",
    "        print('max_num_segments', max_num_segments)\n",
    "        self.embed = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.lstm_cell = nn.LSTMCell(hidden_dim, hidden_dim)\n",
    "\n",
    "        # LSTM output heads.\n",
    "        self.head_z_1 = nn.Linear(hidden_dim, hidden_dim)  # Latents (z).\n",
    "\n",
    "        if latent_dist == 'gaussian':\n",
    "            self.head_z_2 = nn.Linear(hidden_dim, latent_dim * 2)\n",
    "        elif latent_dist == 'concrete':\n",
    "            self.head_z_2 = nn.Linear(hidden_dim, latent_dim)\n",
    "        else:\n",
    "            raise ValueError('Invalid argument for `latent_dist`.')\n",
    "\n",
    "        self.head_b_1 = nn.Linear(hidden_dim, hidden_dim)  # Boundaries (b).\n",
    "        self.head_b_2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Decoder MLP.\n",
    "        self.decode_1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decode_2 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def masked_encode(self, inputs, mask):\n",
    "        \"\"\"Run masked RNN encoder on input sequence.\"\"\"\n",
    "        hidden = get_lstm_initial_state(\n",
    "            inputs.size(0), self.hidden_dim, device=inputs.device)\n",
    "        outputs = []\n",
    "        for step in range(inputs.size(1)):\n",
    "            hidden = self.lstm_cell(inputs[:, step], hidden)\n",
    "            hidden = (mask[:, step, None] * hidden[0],\n",
    "                      mask[:, step, None] * hidden[1])  # Apply mask.\n",
    "            outputs.append(hidden[0])\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "    def get_boundaries(self, encodings, segment_id, lengths):\n",
    "        \"\"\"Get boundaries (b) for a single segment in batch.\"\"\"\n",
    "        if segment_id == self.max_num_segments - 1:\n",
    "            # Last boundary is always placed on last sequence element.\n",
    "            logits_b = None\n",
    "            sample_b = torch.zeros_like(encodings[:, :, 0]).scatter_(\n",
    "                1, lengths.unsqueeze(1) - 1, 1)\n",
    "        else:\n",
    "            hidden = F.relu(self.head_b_1(encodings))\n",
    "            logits_b = self.head_b_2(hidden).squeeze(-1)\n",
    "            # Mask out first position with large neg. value.\n",
    "            neg_inf = torch.ones(\n",
    "                encodings.size(0), 1, device=encodings.device) * NEG_INF\n",
    "            # TODO(tkipf): Mask out padded positions with large neg. value.\n",
    "            logits_b = torch.cat([neg_inf, logits_b[:, 1:]], dim=1)\n",
    "            if self.training:\n",
    "                sample_b = gumbel_softmax_sample(\n",
    "                    logits_b, temp=self.temp_b)\n",
    "            else:\n",
    "                sample_b_idx = torch.argmax(logits_b, dim=1)\n",
    "                sample_b = to_one_hot(sample_b_idx, logits_b.size(1))\n",
    "\n",
    "        return logits_b, sample_b\n",
    "\n",
    "    def get_latents(self, encodings, probs_b):\n",
    "        \"\"\"Read out latents (z) form input encodings for a single segment.\"\"\"\n",
    "        readout_mask = probs_b[:, 1:, None]  # Offset readout by 1 to left.\n",
    "        readout = (encodings[:, :-1] * readout_mask).sum(1)\n",
    "        hidden = F.relu(self.head_z_1(readout))\n",
    "        logits_z = self.head_z_2(hidden)\n",
    "\n",
    "        # Gaussian latents.\n",
    "        if self.latent_dist == 'gaussian':\n",
    "            if self.training:\n",
    "                mu, log_var = torch.split(logits_z, self.latent_dim, dim=1)\n",
    "                sample_z = gaussian_sample(mu, log_var)\n",
    "            else:\n",
    "                sample_z = logits_z[:, :self.latent_dim]\n",
    "\n",
    "        # Concrete / Gumbel softmax latents.\n",
    "        elif self.latent_dist == 'concrete':\n",
    "            if self.training:\n",
    "                sample_z = gumbel_softmax_sample(\n",
    "                    logits_z, temp=self.temp_z)\n",
    "            else:\n",
    "                sample_z_idx = torch.argmax(logits_z, dim=1)\n",
    "                sample_z = utils.to_one_hot(sample_z_idx, logits_z.size(1))\n",
    "        else:\n",
    "            raise ValueError('Invalid argument for `latent_dist`.')\n",
    "\n",
    "        return logits_z, sample_z\n",
    "\n",
    "    def decode(self, sample_z, length):\n",
    "        \"\"\"Decode single time step from latents and repeat over full seq.\"\"\"\n",
    "        hidden = F.relu(self.decode_1(sample_z))\n",
    "        pred = self.decode_2(hidden)\n",
    "        return pred.unsqueeze(1).repeat(1, length, 1)\n",
    "\n",
    "    def get_next_masks(self, all_b_samples):\n",
    "        \"\"\"Get RNN hidden state masks for next segment.\"\"\"\n",
    "        if len(all_b_samples) < self.max_num_segments:\n",
    "            # Product over cumsums (via log->sum->exp).\n",
    "            log_cumsums = list(\n",
    "                map(lambda x:log_cumsum(x, dim=1), all_b_samples))\n",
    "            mask = torch.exp(sum(log_cumsums))\n",
    "            return mask\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "\n",
    "        # Embed inputs.\n",
    "        print(inputs.size())\n",
    "        embeddings = self.embed(inputs)\n",
    "        print(embeddings.size())\n",
    "        # Create initial mask.\n",
    "        mask = torch.ones(\n",
    "            inputs.size(0), inputs.size(1), device=inputs.device)\n",
    "        print(mask.size())\n",
    "        all_b = {'logits': [], 'samples': []}\n",
    "        all_z = {'logits': [], 'samples': []}\n",
    "        all_encs = []\n",
    "        all_recs = []\n",
    "        all_masks = []\n",
    "        for seg_id in range(self.max_num_segments):\n",
    "\n",
    "            # Get masked LSTM encodings of inputs.\n",
    "            #print('mask:', mask[:1])\n",
    "            encodings = self.masked_encode(embeddings, mask)\n",
    "            print(encodings.size())\n",
    "            all_encs.append(encodings)\n",
    "\n",
    "            # Get boundaries (b) for current segment.\n",
    "            logits_b, sample_b = self.get_boundaries(\n",
    "                encodings, seg_id, lengths)\n",
    "            all_b['logits'].append(logits_b)\n",
    "            all_b['samples'].append(sample_b)\n",
    "            print(logits_b.size())\n",
    "            #print(logits_b[:1])\n",
    "            print(sample_b.size())\n",
    "            #print(sample_b[:1])\n",
    "            # Get latents (z) for current segment.\n",
    "            logits_z, sample_z = self.get_latents(\n",
    "                encodings, sample_b)\n",
    "            all_z['logits'].append(logits_z)\n",
    "            all_z['samples'].append(sample_z)\n",
    "\n",
    "            # Get masks for next segment.\n",
    "            mask = self.get_next_masks(all_b['samples'])\n",
    "            print(mask.size())\n",
    "            all_masks.append(mask)\n",
    "\n",
    "            # Decode current segment from latents (z).\n",
    "            reconstructions = self.decode(sample_z, length=inputs.size(1))\n",
    "            all_recs.append(reconstructions)\n",
    "\n",
    "        return all_encs, all_recs, all_masks, all_b, all_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim: 6\n",
      "hidden_dim: 64\n",
      "latent_dim 32\n",
      "max_num_segments 3\n"
     ]
    }
   ],
   "source": [
    "model = CompILE(\n",
    "    input_dim=6,  # +1 for EOS/Padding symbol.\n",
    "    hidden_dim=64,\n",
    "    latent_dim=32,\n",
    "    max_num_segments=3,\n",
    "    latent_dist='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 13])\n",
      "torch.Size([512, 13, 64])\n",
      "torch.Size([512, 13])\n",
      "torch.Size([512, 13, 64])\n",
      "torch.Size([512, 13])\n",
      "torch.Size([512, 13])\n",
      "torch.Size([512, 13])\n",
      "torch.Size([512, 13, 64])\n",
      "torch.Size([512, 13])\n",
      "torch.Size([512, 13])\n",
      "torch.Size([512, 13])\n",
      "torch.Size([512, 13, 64])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-091956df9146>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-c2da518c83c8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, lengths)\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[0mall_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mall_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'samples'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m             \u001b[1;31m#print(logits_b[:1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "outputs = model.forward(inputs, lengths)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "527a93331b4b1a8345148922acc34427fb7591433d63b66d32040b6fbbc6d593"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
