{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_scatter import scatter_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(size, tensor):\n",
    "    bound = 1.0 / math.sqrt(size)\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "def load_data(file_path):\n",
    "    '''\n",
    "        argument:\n",
    "            file_path: ./data/FB15k-237\n",
    "        \n",
    "        return:\n",
    "            entity2id, relation2id, train_triplets, valid_triplets, test_triplets\n",
    "    '''\n",
    "\n",
    "    print(\"load data from {}\".format(file_path))\n",
    "\n",
    "    with open(os.path.join(file_path, 'entities.dict')) as f:\n",
    "        entity2id = dict()\n",
    "\n",
    "        for line in f:\n",
    "            eid, entity = line.strip().split('\\t')\n",
    "            entity2id[entity] = int(eid)\n",
    "\n",
    "    with open(os.path.join(file_path, 'relations.dict')) as f:\n",
    "        relation2id = dict()\n",
    "\n",
    "        for line in f:\n",
    "            rid, relation = line.strip().split('\\t')\n",
    "            relation2id[relation] = int(rid)\n",
    "\n",
    "    train_triplets = read_triplets(os.path.join(file_path, 'train.txt'), entity2id, relation2id)\n",
    "    valid_triplets = read_triplets(os.path.join(file_path, 'valid.txt'), entity2id, relation2id)\n",
    "    test_triplets = read_triplets(os.path.join(file_path, 'test.txt'), entity2id, relation2id)\n",
    "\n",
    "    print('num_entity: {}'.format(len(entity2id)))\n",
    "    print('num_relation: {}'.format(len(relation2id)))\n",
    "    print('num_train_triples: {}'.format(len(train_triplets)))\n",
    "    print('num_valid_triples: {}'.format(len(valid_triplets)))\n",
    "    print('num_test_triples: {}'.format(len(test_triplets)))\n",
    "\n",
    "    return entity2id, relation2id, train_triplets, valid_triplets, test_triplets\n",
    "\n",
    "def read_triplets(file_path, entity2id, relation2id):\n",
    "    triplets = []\n",
    "\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            head, relation, tail = line.strip().split('\\t')\n",
    "            triplets.append((entity2id[head], relation2id[relation], entity2id[tail]))\n",
    "\n",
    "    return np.array(triplets)\n",
    "\n",
    "def sample_edge_uniform(n_triples, sample_size):\n",
    "    \"\"\"Sample edges uniformly from all the edges.\"\"\"\n",
    "    all_edges = np.arange(n_triples)\n",
    "    return np.random.choice(all_edges, sample_size, replace=False)\n",
    "\n",
    "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
    "    size_of_batch = len(pos_samples)\n",
    "    num_to_generate = size_of_batch * negative_rate\n",
    "    neg_samples = np.tile(pos_samples, (negative_rate, 1))\n",
    "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
    "    labels[: size_of_batch] = 1\n",
    "    values = np.random.choice(num_entity, size=num_to_generate)\n",
    "    choices = np.random.uniform(size=num_to_generate)\n",
    "    subj = choices > 0.5\n",
    "    obj = choices <= 0.5\n",
    "    neg_samples[subj, 0] = values[subj]\n",
    "    neg_samples[obj, 2] = values[obj]\n",
    "\n",
    "    return np.concatenate((pos_samples, neg_samples)), labels\n",
    "\n",
    "def edge_normalization(edge_type, edge_index, num_entity, num_relation):\n",
    "    '''\n",
    "        Edge normalization trick\n",
    "        - one_hot: (num_edge, num_relation)\n",
    "        - deg: (num_node, num_relation)\n",
    "        - index: (num_edge)\n",
    "        - deg[edge_index[0]]: (num_edge, num_relation)\n",
    "        - edge_norm: (num_edge)\n",
    "    '''\n",
    "    print(edge_type[:30])\n",
    "    one_hot = F.one_hot(edge_type, num_classes = 2 * num_relation).to(torch.float)\n",
    "    #print(one_hot.size())\n",
    "    #print(edge_index.size())    \n",
    "    # 得到每一个起点节点对应的关系类型\n",
    "    deg = scatter_add(one_hot, edge_index[0], dim = 0, dim_size = num_entity)\n",
    "    #print(deg.size())\n",
    "    #print(deg.sum())\n",
    "    index = edge_type + torch.arange(len(edge_index[0])) * (2 * num_relation)\n",
    "    #print(index.size())\n",
    "    # 边归一化，实现的是公式里1/c这个部分\n",
    "    edge_norm = 1 / deg[edge_index[0]].view(-1)[index]\n",
    "\n",
    "    return edge_norm\n",
    "\n",
    "def generate_sampled_graph_and_labels(triplets, sample_size, split_size, num_entity, num_rels, negative_rate):\n",
    "    \"\"\"\n",
    "        Get training graph and signals\n",
    "        First perform edge neighborhood sampling on graph, then perform negative\n",
    "        sampling to generate negative samples\n",
    "    \"\"\"\n",
    "\n",
    "    edges = sample_edge_uniform(len(triplets), sample_size)\n",
    "\n",
    "    # Select sampled edges\n",
    "    edges = triplets[edges]\n",
    "    src, rel, dst = edges.transpose()\n",
    "    uniq_entity, edges = np.unique((src, dst), return_inverse=True)\n",
    "    src, dst = np.reshape(edges, (2, -1))\n",
    "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
    "\n",
    "    # Negative sampling\n",
    "    samples, labels = negative_sampling(relabeled_edges, len(uniq_entity), negative_rate)\n",
    "\n",
    "    # further split graph, only half of the edges will be used as graph\n",
    "    # structure, while the rest half is used as unseen positive samples\n",
    "    split_size = int(sample_size * split_size)\n",
    "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
    "                                       size=split_size, replace=False)\n",
    "\n",
    "    src = torch.tensor(src[graph_split_ids], dtype = torch.long).contiguous()\n",
    "    dst = torch.tensor(dst[graph_split_ids], dtype = torch.long).contiguous()\n",
    "    rel = torch.tensor(rel[graph_split_ids], dtype = torch.long).contiguous()\n",
    "\n",
    "    # Create bi-directional graph\n",
    "    src, dst = torch.cat((src, dst)), torch.cat((dst, src))\n",
    "    rel = torch.cat((rel, rel + num_rels))\n",
    "\n",
    "    edge_index = torch.stack((src, dst))\n",
    "    edge_type = rel\n",
    "\n",
    "    data = Data(edge_index = edge_index)\n",
    "    data.entity = torch.from_numpy(uniq_entity)\n",
    "    data.edge_type = edge_type\n",
    "    data.edge_norm = edge_normalization(edge_type, edge_index, len(uniq_entity), num_rels)\n",
    "    data.samples = torch.from_numpy(samples)\n",
    "    data.labels = torch.from_numpy(labels)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from utils import uniform\n",
    "\n",
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, num_bases, dropout):\n",
    "        super(RGCN, self).__init__()\n",
    "\n",
    "        self.entity_embedding = nn.Embedding(num_entities, 100)\n",
    "        self.relation_embedding = nn.Parameter(torch.Tensor(num_relations, 100))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_embedding, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.conv1 = RGCNConv(\n",
    "            100, 100, num_relations * 2, num_bases=num_bases)\n",
    "        self.conv2 = RGCNConv(\n",
    "            100, 100, num_relations * 2, num_bases=num_bases)\n",
    "\n",
    "        self.dropout_ratio = dropout\n",
    "\n",
    "    def forward(self, entity, edge_index, edge_type, edge_norm):\n",
    "        x = self.entity_embedding(entity)\n",
    "        #print(entity.size())\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_type, edge_norm))\n",
    "        x = F.dropout(x, p = self.dropout_ratio, training = self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type, edge_norm)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def distmult(self, embedding, triplets):\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.relation_embedding[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def score_loss(self, embedding, triplets, target):\n",
    "        score = self.distmult(embedding, triplets)\n",
    "\n",
    "        return F.binary_cross_entropy_with_logits(score, target)\n",
    "\n",
    "    def reg_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.relation_embedding.pow(2))\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        num_bases (int): Number of bases used for basis-decomposition.\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_relations, num_bases,\n",
    "                 root_weight=True, bias=True, **kwargs):\n",
    "        super(RGCNConv, self).__init__(aggr='mean', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "\n",
    "        self.basis = nn.Parameter(torch.Tensor(num_bases, in_channels, out_channels))\n",
    "        self.att = nn.Parameter(torch.Tensor(num_relations, num_bases))\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.num_bases * self.in_channels\n",
    "        uniform(size, self.basis)\n",
    "        uniform(size, self.att)\n",
    "        uniform(size, self.root)\n",
    "        uniform(size, self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type, edge_norm=None, size=None):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.propagate(edge_index, size=size, x=x, edge_type=edge_type,\n",
    "                              edge_norm=edge_norm)\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_index_j, edge_type, edge_norm):\n",
    "        w = torch.matmul(self.att, self.basis.view(self.num_bases, -1))\n",
    "\n",
    "        # If no node features are given, we implement a simple embedding\n",
    "        # loopkup based on the target node index and its edge type.\n",
    "        if x_j is None:\n",
    "            #print(1)\n",
    "            w = w.view(-1, self.out_channels)\n",
    "            index = edge_type * self.in_channels + edge_index_j\n",
    "            out = torch.index_select(w, 0, index)\n",
    "        else:\n",
    "            #print(0)\n",
    "            w = w.view(self.num_relations, self.in_channels, self.out_channels)\n",
    "            #print(w.size())\n",
    "            #print(edge_type.size())\n",
    "            w = torch.index_select(w, 0, edge_type)\n",
    "            #print(w.size())\n",
    "            #print(x_j.size())\n",
    "            #print(x_j.unsqueeze(1).size())\n",
    "            #print(torch.bmm(x_j.unsqueeze(1), w).size())\n",
    "            out = torch.bmm(x_j.unsqueeze(1), w).squeeze(-2)\n",
    "            #print(out.size())\n",
    "\n",
    "        return out if edge_norm is None else out * edge_norm.view(-1, 1)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        if self.root is not None:\n",
    "            if x is None:\n",
    "                #print(1)\n",
    "                out = aggr_out + self.root\n",
    "            else:\n",
    "                #print(0)\n",
    "                out = aggr_out + torch.matmul(x, self.root)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, num_relations={})'.format(\n",
    "            self.__class__.__name__, self.in_channels, self.out_channels,\n",
    "            self.num_relations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from ./data/FB15k-237\n",
      "num_entity: 14541\n",
      "num_relation: 237\n",
      "num_train_triples: 272115\n",
      "num_valid_triples: 17535\n",
      "num_test_triples: 20466\n"
     ]
    }
   ],
   "source": [
    "entity2id, relation2id, train_triplets, valid_triplets, test_triplets = load_data('./data/FB15k-237')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_triplets = torch.LongTensor(np.concatenate((train_triplets, valid_triplets, test_triplets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_triplets, model, use_cuda, batch_size, split_size, negative_sample, reg_ratio, num_entities, num_relations):\n",
    "\n",
    "    train_data = generate_sampled_graph_and_labels(train_triplets, batch_size, split_size, num_entities, num_relations, negative_sample)\n",
    "\n",
    "    # if use_cuda:\n",
    "    #     device = torch.device('cuda')\n",
    "    #     train_data.to(device)\n",
    "\n",
    "    entity_embedding = model(train_data.entity, train_data.edge_index, train_data.edge_type, train_data.edge_norm)\n",
    "    loss = model.score_loss(entity_embedding, train_data.samples, train_data.labels) + reg_ratio * model.reg_loss(entity_embedding)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RGCN(len(entity2id), len(relation2id), num_bases=4, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 82, 199, 166, 125, 186, 202,  64,  59, 123, 125,   9, 124,  82,  75,\n",
      "        125,   3, 140, 214, 205, 214, 114,  12,  31,  58,  59, 116,  31, 163,\n",
      "         59, 215])\n",
      "tensor([148, 125, 215,  45,  75,  91, 114,  59,  50,  61,  11, 111, 128,  59,\n",
      "        179, 204, 128,  17, 179,  11,  75,   2, 215,  54,   2, 125, 125,   4,\n",
      "         54, 121])\n",
      "tensor([  9, 126, 199,   2,  75,   2,  11,  59,  56, 126, 125,  10, 226, 105,\n",
      "         11, 230, 108,  11,   9, 120, 213,  41,  39,   2, 193, 176, 125,  58,\n",
      "        110, 125])\n",
      "tensor([108,  82,  17, 121,  95,  31, 213,  59, 128, 128,  54, 125, 198, 223,\n",
      "        110,   2, 151,  17, 196, 165,   2, 215,  88, 213, 125, 108,  58, 123,\n",
      "         10,  61])\n",
      "tensor([126, 111,  75, 216, 204, 215, 199,  54, 179, 198,   4, 215,  11, 215,\n",
      "         11,  54,   9,   4, 108, 204, 114,  87, 108, 126, 124,   2,   2,  11,\n",
      "         21, 148])\n",
      "tensor([198,  59, 214, 204,  31,  53, 229,   1,   4, 125,  11,  88, 148, 198,\n",
      "        166, 198,  11, 204, 143,  59, 111,  74, 114,  11,   9,  55, 128, 180,\n",
      "         11,  82])\n",
      "tensor([145,   4, 174,  82, 187,  59, 155,   9,  98,  61, 148, 133, 125, 166,\n",
      "         11,  75,  62,  82,  10,  75,   7, 171,   9, 147,  11,  31,  87, 159,\n",
      "        186, 110])\n",
      "tensor([  4,  21, 170,   9,  11, 111, 148,   2, 179, 121,  39,  82, 105, 125,\n",
      "         74,  82, 125,  58,  55, 111, 126, 184, 173,   2,   9, 186,  11, 126,\n",
      "         16, 193])\n",
      "tensor([172,  83,  75, 125,  17,  10, 179, 113,  42,  54, 128, 125, 215,  54,\n",
      "         75,   7, 166,   2,  11, 105, 128, 110, 140, 111,  30, 186, 173,  75,\n",
      "        108, 128])\n",
      "tensor([136, 214,   4, 125,  61, 120,  86,  10,  22,  45, 155, 120, 194,  32,\n",
      "        128, 179,  78,  82,  37,  13,  61,  32,  75, 148,  47, 124,  75,  75,\n",
      "        125, 179])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = train(train_triplets, model, use_cuda=-1, batch_size=30000, split_size=0.5, \n",
    "                negative_sample=1, reg_ratio = 1e-2, num_entities=len(entity2id), num_relations=len(relation2id))\n",
    "            \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-43-1b8bddcdeaee>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src = torch.tensor(src, dtype = torch.long).contiguous()\n",
      "<ipython-input-43-1b8bddcdeaee>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dst = torch.tensor(dst, dtype = torch.long).contiguous()\n",
      "<ipython-input-43-1b8bddcdeaee>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rel = torch.tensor(rel, dtype = torch.long).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([183, 124, 166,  11, 225,  54,  82, 108, 215,  49,  80, 187,  21,  75,\n",
      "         17, 215,  31,  82,  37, 215, 108,   2, 196, 125,  59, 173,  59,  39,\n",
      "         75, 230])\n"
     ]
    }
   ],
   "source": [
    "def build_test_graph(num_nodes, num_rels, triplets):\n",
    "    src, rel, dst = triplets.transpose()\n",
    "\n",
    "    src = torch.from_numpy(src)\n",
    "    rel = torch.from_numpy(rel)\n",
    "    dst = torch.from_numpy(dst)\n",
    "\n",
    "    src, dst = torch.cat((src, dst)), torch.cat((dst, src))\n",
    "    rel = torch.cat((rel, rel + num_rels))\n",
    "\n",
    "    src = torch.tensor(src, dtype = torch.long).contiguous()\n",
    "    dst = torch.tensor(dst, dtype = torch.long).contiguous()\n",
    "    rel = torch.tensor(rel, dtype = torch.long).contiguous()\n",
    "\n",
    "    edge_index = torch.stack((src, dst))\n",
    "    edge_type = rel\n",
    "    \n",
    "    #print(rel[:30])\n",
    "\n",
    "    data = Data(edge_index = edge_index)\n",
    "    data.entity = torch.from_numpy(np.arange(num_nodes))\n",
    "    data.edge_type = edge_type\n",
    "    data.edge_norm = edge_normalization(edge_type, edge_index, num_nodes, num_rels)\n",
    "\n",
    "    return data\n",
    "    \n",
    "test_graph = build_test_graph(len(entity2id), len(relation2id), train_triplets[:27000])\n",
    "test_triplets = torch.LongTensor(test_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triplets = test_triplets[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embedding = model(test_graph.entity, test_graph.edge_index, test_graph.edge_type, test_graph.edge_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_triplets, model, test_graph, all_triplets):\n",
    "\n",
    "    mrr = calc_mrr(entity_embedding, model.relation_embedding, test_triplets, all_triplets, hits=[1, 3, 10])\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_rank(score, target):\n",
    "    _, indices = torch.sort(score, dim=1, descending=True)\n",
    "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
    "    indices = indices[:, 1].view(-1)\n",
    "    print(indices)\n",
    "    return indices\n",
    "\n",
    "# return MRR (filtered), and Hits @ (1, 3, 10)\n",
    "def calc_mrr(embedding, w, test_triplets, all_triplets, hits=[]):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        num_entity = len(embedding)\n",
    "\n",
    "        ranks_s = []\n",
    "        ranks_o = []\n",
    "\n",
    "        head_relation_triplets = all_triplets[:, :2]\n",
    "        tail_relation_triplets = torch.stack((all_triplets[:, 2], all_triplets[:, 1])).transpose(0, 1)\n",
    "\n",
    "        for test_triplet in tqdm(test_triplets):\n",
    "\n",
    "            # Perturb object\n",
    "            subject = test_triplet[0]\n",
    "            relation = test_triplet[1]\n",
    "            object_ = test_triplet[2]\n",
    "\n",
    "            subject_relation = test_triplet[:2]\n",
    "            delete_index = torch.sum(head_relation_triplets == subject_relation, dim = 1)\n",
    "            delete_index = torch.nonzero(delete_index == 2).squeeze()\n",
    "\n",
    "            delete_entity_index = all_triplets[delete_index, 2].view(-1).numpy()\n",
    "            perturb_entity_index = np.array(list(set(np.arange(num_entity)) - set(delete_entity_index)))\n",
    "            perturb_entity_index = torch.from_numpy(perturb_entity_index)\n",
    "            perturb_entity_index = torch.cat((perturb_entity_index, object_.view(-1)))\n",
    "            \n",
    "            emb_ar = embedding[subject] * w[relation]\n",
    "            emb_ar = emb_ar.view(-1, 1, 1)\n",
    "            print('emb_ar:', emb_ar.size())\n",
    "\n",
    "            emb_c = embedding[perturb_entity_index]\n",
    "            emb_c = emb_c.transpose(0, 1).unsqueeze(1)\n",
    "            print('emb_c:', emb_c.size())\n",
    "\n",
    "            out_prod = torch.bmm(emb_ar, emb_c)\n",
    "            print('out_prod:', out_prod.size())\n",
    "            score = torch.sum(out_prod, dim = 0)\n",
    "            score = torch.sigmoid(score)\n",
    "            print('score:', score.size())\n",
    "            \n",
    "            target = torch.tensor(len(perturb_entity_index) - 1)\n",
    "            print('target', target)\n",
    "            ranks_s.append(sort_and_rank(score, target))\n",
    "            return \n",
    "\n",
    "            # Perturb subject\n",
    "            object_ = test_triplet[2]\n",
    "            relation = test_triplet[1]\n",
    "            subject = test_triplet[0]\n",
    "\n",
    "            object_relation = torch.tensor([object_, relation])\n",
    "            delete_index = torch.sum(tail_relation_triplets == object_relation, dim = 1)\n",
    "            delete_index = torch.nonzero(delete_index == 2).squeeze()\n",
    "\n",
    "            delete_entity_index = all_triplets[delete_index, 0].view(-1).numpy()\n",
    "            perturb_entity_index = np.array(list(set(np.arange(num_entity)) - set(delete_entity_index)))\n",
    "            perturb_entity_index = torch.from_numpy(perturb_entity_index)\n",
    "            perturb_entity_index = torch.cat((perturb_entity_index, subject.view(-1)))\n",
    "\n",
    "            emb_ar = embedding[object_] * w[relation]\n",
    "            emb_ar = emb_ar.view(-1, 1, 1)\n",
    "\n",
    "            emb_c = embedding[perturb_entity_index]\n",
    "            emb_c = emb_c.transpose(0, 1).unsqueeze(1)\n",
    "\n",
    "            out_prod = torch.bmm(emb_ar, emb_c)\n",
    "            score = torch.sum(out_prod, dim = 0)\n",
    "            score = torch.sigmoid(score)\n",
    "\n",
    "            target = torch.tensor(len(perturb_entity_index) - 1)\n",
    "            ranks_o.append(sort_and_rank(score, target))\n",
    "\n",
    "        ranks_s = torch.cat(ranks_s)\n",
    "        ranks_o = torch.cat(ranks_o)\n",
    "\n",
    "        ranks = torch.cat([ranks_s, ranks_o])\n",
    "        ranks += 1 # change to 1-indexed\n",
    "\n",
    "        mrr = torch.mean(1.0 / ranks.float())\n",
    "        print(\"MRR (filtered): {:.6f}\".format(mrr.item()))\n",
    "\n",
    "        for hit in hits:\n",
    "            avg_count = torch.mean((ranks <= hit).float())\n",
    "            print(\"Hits (filtered) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
    "            \n",
    "    return mrr.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_ar: torch.Size([100, 1, 1])\n",
      "emb_c: torch.Size([100, 1, 14530])\n",
      "out_prod: torch.Size([100, 1, 14530])\n",
      "score: torch.Size([1, 14530])\n",
      "target tensor(14529)\n",
      "tensor([441])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(test_triplets, model, test_graph, all_triplets)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "527a93331b4b1a8345148922acc34427fb7591433d63b66d32040b6fbbc6d593"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
